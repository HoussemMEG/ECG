{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T10:27:50.475832Z",
     "start_time": "2023-05-10T10:27:46.118387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[34m\n",
      "Sessions: 2800 per category _ 35 freq _ long window\u001B[0m\n",
      " Data case: \u001B[1mevoked\u001B[0m\u001B[0m\n",
      " Version: \u001B[1m1\u001B[0m\u001B[0m\n",
      " Alpha: \u001B[1m[0.00016, 0.00018, 0.00012, 0.00016, 0.00011, 0.00013, 0.00017, 0.00021, 0.00026, 0.00034, 0.00032, 0.00025]\u001B[0m\u001B[0m\n",
      " Model frequencies: [ 3.    4.24  5.47  6.71  7.94  9.18 10.41 11.65 12.88 14.12 15.35 16.59\n",
      " 17.82 19.06 20.29 21.53 22.76 24.   25.24 26.47 27.71 28.94 30.18 31.41\n",
      " 32.65 33.88 35.12 36.35 37.59 38.82 40.06 41.29 42.53 43.76 45.  ]\n",
      " N_freq = \u001B[1m35\u001B[0m\u001B[0m\n",
      " N_point = \u001B[1m240\u001B[0m\u001B[0m\n",
      " Beta_dim = \u001B[1m8365\u001B[0m\u001B[0m\n",
      " Parsimony: [0.04 0.08 0.12 0.16 0.2  0.24 0.28 0.32 0.36 0.4  0.44 0.48 0.52 0.56\n",
      " 0.6  0.64 0.68 0.72 0.76 0.8  0.84 0.88 0.92 0.96 1.  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "\n",
    "from reader import Reader\n",
    "from utils import print_c, read_parameters, convolution2d\n",
    "\n",
    "from plot import Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "selection = {'lead': [1],\n",
    "             'parsimony': [-1],\n",
    "             'time': (40, 76),\n",
    "             'frequency': (0, 25),\n",
    "             'merge_pars':False,\n",
    "             'merge_lead':False}\n",
    "\n",
    "conv2d = {'do_filter': False,\n",
    "          'type': 'gaussian',\n",
    "            'l': 5,\n",
    "            'sigma': 2}\n",
    "\n",
    "# observations:\n",
    "#   20 freq\n",
    "#       1dAVb  (57, 78, 0, 15) lead: 1 5 7 9 10 11\n",
    "#       SB     (57, 76, 0, 12) lead: 8(12) 7(15) 10(12 + 80)\n",
    "#       ST     (60, 76, 3, 15) lead: 8\n",
    "#\n",
    "#   30 freq\n",
    "#       1dAVb  (40, 76, 0, 25) lead: 1 2 5 9 10\n",
    "#       SB     (40, 76, 0, 25) lead: 10 et (50, 78, 0, 16) lead: 7 8\n",
    "#       ST     () lead: / RIEN\n",
    "\n",
    "\n",
    "# condition = ['1dAVb', 'RBBB', 'LBBB', 'SB', 'AF', 'ST', 'HEALTHY']\n",
    "condition = ['1dAVb', 'HEALTHY']\n",
    "\n",
    "# session = \"2800 per category _ 20 freq _ short window\"\n",
    "session = \"2800 per category _ 35 freq _ long window\"\n",
    "\n",
    "_use_all_data = True\n",
    "parameter = read_parameters(session)\n",
    "\n",
    "def _feature_selection(x, selection, conv2d, parameter):\n",
    "    \"\"\"\n",
    "    Method to perform the feature selection based on the self._selection dictionary.\n",
    "        Select leads in <selection['lead']> if empty no selection is performed\n",
    "        Select parsimony in <selection['pars']> if empty no selection is performed\n",
    "        Select time in <selection['time'] = (t_min, t_max)> if t_min = None, no selection will be performed on\n",
    "            left side of time. Same goes for t_max\n",
    "        Select Frequency in <selection['frequency'] = (f_min, f_max)> if f_max = None, no selection will be\n",
    "            performed on upper frequency. Same goes for f_min. If float is given, select directly, if fload are\n",
    "            provided select based on model frequency.\n",
    "        Merge all parsimony levels if <selection['merge_pars'] = True>, else nothing is changed\n",
    "        Merge all leads if <selection['merge_lead'] = True>, else nothing is changed\n",
    "    :param x: input data array with shape (n_exams, n_leads, n_features, n_parsimony)\n",
    "    :return: x: output data after selection, /!\\ shape has been changed to (n_exams, n_leads, n_parsimony, n_features)\n",
    "    \"\"\"\n",
    "    x = x.transpose((0, 1, 3, 2))\n",
    "\n",
    "    # Lead selection\n",
    "    if selection['lead']:\n",
    "        x = x[:, [*selection['lead']], ...]\n",
    "\n",
    "    # Parsimony level selection\n",
    "    if selection['parsimony']:\n",
    "        x = x[:, :, [*selection['parsimony']], ...]\n",
    "\n",
    "    # Time or/and frequency selection\n",
    "    if any(selection['time']) or any(selection['frequency']):\n",
    "        # temps shape: (n_exams, n_leads, n_parsimony, time, frequency)\n",
    "        temp = np.array(np.split(x, parameter['n_point'] - 1, axis=-1)).transpose((1, 2, 3, 0, 4))\n",
    "\n",
    "        ## IDEA of gaussian convolution\n",
    "        if conv2d['do_filter']:\n",
    "            for exam_i in range(len(temp)):\n",
    "                temp[exam_i, 0, 0, :, :] = convolution2d(temp[exam_i, 0, 0, :, :], kernel_type=conv2d['type'],\n",
    "                                                         l=conv2d['l'], sigma=conv2d['sigma'])\n",
    "\n",
    "        if any(selection['time']):\n",
    "            t_min, t_max = get_time_indexes(selection, parameter)\n",
    "            temp = temp[:, :, :, t_min:t_max, ...]\n",
    "\n",
    "        if any(selection['frequency']):\n",
    "            f_min, f_max = get_freq_indexes(selection, parameter)\n",
    "            temp = temp[..., f_min:f_max]\n",
    "        x = np.reshape(temp, (temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3] * temp.shape[4]))\n",
    "\n",
    "    # Merge parsimony indexes\n",
    "    if selection['merge_pars'] and x.shape[2] > 1:\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2] * x.shape[3]))[:, :, np.newaxis, :]\n",
    "\n",
    "    # Merge leads\n",
    "    if selection['merge_lead'] and x.shape[1] > 1:\n",
    "        x = x.transpose((0, 2, 1, 3))\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2] * x.shape[3]))[:, :, np.newaxis, :]\n",
    "        x = x.transpose((0, 2, 1, 3))\n",
    "\n",
    "    return x\n",
    "\n",
    "def _init_x_y(reader_hdf5, selection):\n",
    "    n_exams, n_leads, n_features, n_pars = next(reader_hdf5)\n",
    "\n",
    "    if selection['lead']:\n",
    "        n_leads = len(selection['lead'])\n",
    "    if selection['parsimony']:\n",
    "        n_pars = len(selection['parsimony'])\n",
    "\n",
    "    if any(selection['time']) or any(selection['frequency']):\n",
    "        t_min, t_max = get_time_indexes(selection, parameter)\n",
    "        f_min, f_max = get_freq_indexes(selection, parameter)\n",
    "        n_features = (f_max - f_min) * (t_max - t_min)\n",
    "\n",
    "    if selection['merge_lead'] and n_leads > 1:\n",
    "        n_features *= n_leads\n",
    "        n_leads = 1\n",
    "\n",
    "    if selection['merge_pars'] and n_pars > 1:\n",
    "        n_features *= n_pars\n",
    "        n_pars = 1\n",
    "\n",
    "    return np.zeros((n_exams, n_leads, n_pars, n_features)), np.empty((n_exams,), dtype=np.int8)\n",
    "\n",
    "def get_time_indexes(selection, parameter):\n",
    "    t_min = selection['time'][0] if selection['time'][0] is not None else 0\n",
    "    t_max = selection['time'][1] if selection['time'][1] is not None else parameter['n_point'] - 1\n",
    "    return t_min, t_max\n",
    "\n",
    "def get_freq_indexes(selection, parameter):\n",
    "    f_min = selection['frequency'][0] if selection['frequency'][0] is not None else 0\n",
    "    f_max = selection['frequency'][1] if selection['frequency'][1] is not None else parameter['n_freq']\n",
    "    if isinstance(f_min, float):\n",
    "        f_min = np.argmin(parameter['model_freq'] - f_min)\n",
    "    if isinstance(f_max, float):\n",
    "        f_max = np.argmin(parameter['model_freq'] - f_max)\n",
    "    return f_min, f_max\n",
    "\n",
    "def _score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the score for a classification task based on the specified metric.\n",
    "\n",
    "    :param y_true: True labels for the input samples, shape (n_samples,).\n",
    "    :param y_pred: Predicted labels for the input samples, shape (n_samples,)\n",
    "\n",
    "    :return: Score for the specified metric, multiplied by 100 to convert to a percentage.\n",
    "    :raises ValueError: if the specified metric is not one of ['accuracy', 'f1'].\n",
    "    \"\"\"\n",
    "    # List of available metrics\n",
    "    all_metrics = ['accuracy', 'f1']\n",
    "\n",
    "    ## testing CNN\n",
    "    for i, val in enumerate(y_pred):\n",
    "        if val[0] > val[1]:\n",
    "            y_pred[i][0] = 1\n",
    "            y_pred[i][1] = 0\n",
    "        else:\n",
    "            y_pred[i][0] = 0\n",
    "            y_pred[i][1] = 1\n",
    "    y_pred = [np.where(r==1)[0][0] for r in y_pred]\n",
    "    y_true = [np.where(r==1)[0][0] for r in y_true]\n",
    "\n",
    "    return accuracy_score(y_true, y_pred) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[34mtrain\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch  1/28: 100%|██████████| 199/199 [00:02<00:00, 82.93it/s]\n",
      "Batch  2/28: 100%|██████████| 199/199 [00:02<00:00, 86.64it/s]\n",
      "Batch  3/28: 100%|██████████| 199/199 [00:02<00:00, 81.34it/s]\n",
      "Batch  4/28: 100%|██████████| 199/199 [00:02<00:00, 90.76it/s]\n",
      "Batch  5/28: 100%|██████████| 199/199 [00:02<00:00, 86.61it/s]\n",
      "Batch  6/28: 100%|██████████| 199/199 [00:02<00:00, 82.03it/s]\n",
      "Batch  7/28: 100%|██████████| 199/199 [00:02<00:00, 84.86it/s]\n",
      "Batch  8/28: 100%|██████████| 199/199 [00:02<00:00, 82.28it/s]\n",
      "Batch  9/28: 100%|██████████| 199/199 [00:02<00:00, 87.60it/s]\n",
      "Batch 10/28: 100%|██████████| 199/199 [00:02<00:00, 87.55it/s]\n",
      "Batch 11/28: 100%|██████████| 199/199 [00:02<00:00, 90.35it/s]\n",
      "Batch 12/28: 100%|██████████| 199/199 [00:02<00:00, 90.50it/s]\n",
      "Batch 13/28: 100%|██████████| 199/199 [00:02<00:00, 93.59it/s]\n",
      "Batch 14/28: 100%|██████████| 199/199 [00:02<00:00, 91.15it/s]\n",
      "Batch 15/28: 100%|██████████| 198/198 [00:02<00:00, 93.74it/s]\n",
      "Batch 16/28: 100%|██████████| 198/198 [00:02<00:00, 92.67it/s]\n",
      "Batch 17/28: 100%|██████████| 198/198 [00:02<00:00, 93.19it/s]\n",
      "Batch 18/28: 100%|██████████| 198/198 [00:02<00:00, 91.97it/s]\n",
      "Batch 19/28: 100%|██████████| 198/198 [00:02<00:00, 91.33it/s]\n",
      "Batch 20/28: 100%|██████████| 198/198 [00:02<00:00, 92.92it/s]\n",
      "Batch 21/28: 100%|██████████| 198/198 [00:02<00:00, 92.69it/s]\n",
      "Batch 22/28: 100%|██████████| 198/198 [00:02<00:00, 91.63it/s]\n",
      "Batch 23/28: 100%|██████████| 198/198 [00:02<00:00, 91.87it/s]\n",
      "Batch 24/28: 100%|██████████| 198/198 [00:02<00:00, 93.36it/s]\n",
      "Batch 25/28: 100%|██████████| 198/198 [00:02<00:00, 93.68it/s]\n",
      "Batch 26/28: 100%|██████████| 198/198 [00:02<00:00, 91.61it/s]\n",
      "Batch 27/28: 100%|██████████| 198/198 [00:02<00:00, 92.36it/s]\n",
      "Batch 28/28: 100%|██████████| 198/198 [00:02<00:00, 91.04it/s]\n"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "batch_size = 100\n",
    "\n",
    "print_c(f'{mode}', 'blue', bold=True)\n",
    "mode_ = 'learning' if mode.lower() == 'train' else 'evaluation'\n",
    "\n",
    "# Init reader and x, y for the case of use_all_data = True\n",
    "reader = Reader(batch_size=batch_size * len(condition))\n",
    "hdf5_batch_iterator = reader.read_hdf5(session, mode, condition, random=False, verbose=True)\n",
    "x, y = _init_x_y(hdf5_batch_iterator, selection)\n",
    "\n",
    "pointer = 0\n",
    "for x_batch, y_batch in hdf5_batch_iterator:\n",
    "    x_batch = _feature_selection(x_batch, selection, conv2d, parameter)  # (n_exams, n_leads, n_pars, n_features)\n",
    "    x[pointer:pointer + len(x_batch)] = x_batch\n",
    "    y[pointer:pointer + len(x_batch)] = y_batch\n",
    "    pointer += len(x_batch)\n",
    "\n",
    "    # x = x_batch\n",
    "    # y = y_batch\n",
    "    # break\n",
    "\n",
    "# lighted ram load\n",
    "del x_batch, y_batch\n",
    "\n",
    "# Aggregate all exams in one batch (can only be possible if there is selection or low memory usage)\n",
    "x = x[:, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_clf(selection, _selection, conv2d):\n",
    "    \"\"\"\n",
    "    Method to perform the selection of the classifier.\n",
    "    Available classifiers are:\n",
    "        'LDA': Linear Discriminant Analysis\n",
    "        'QDA': Quadratic Discriminant Analysis\n",
    "        'RF':  Random forest\n",
    "        'SGD': Stochastic Gradient Descent\n",
    "        'PA':  Passive Agressive Classifier\n",
    "    :param selection: 'str' to choose the returned classifier.\n",
    "    :return: clf\n",
    "    \"\"\"\n",
    "    # param = {'classifier': self._clf_choice}\n",
    "    # print(f'selection={self._selection},   conv2d={self._conv2d},   classifier={param}')\n",
    "    if selection == 'LDA':\n",
    "        return LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, covariance_estimator=None,\n",
    "                                          n_components=1)\n",
    "    elif selection == 'QDA':\n",
    "        return QuadraticDiscriminantAnalysis()\n",
    "    elif selection == 'RF':\n",
    "        max_depth = 20\n",
    "        max_leaf_nodes = 200\n",
    "        param = {'classifier': selection, 'max_depth': max_depth, 'max_leaf_nodes': max_leaf_nodes}\n",
    "        print(f'selection={_selection},   conv2d={conv2d},   classifier={param}')\n",
    "        return RandomForestClassifier(max_depth=max_depth, max_leaf_nodes=max_leaf_nodes, warm_start=False, random_state=42, n_jobs=-1)\n",
    "    elif selection == 'SGD':\n",
    "        return SGDClassifier()\n",
    "    elif selection == 'PA':\n",
    "        return PassiveAggressiveClassifier(n_jobs=-1)\n",
    "    elif selection == 'CNN':\n",
    "        input_shape = (_selection['time'][1]-_selection['time'][0],\n",
    "                       _selection['frequency'][1]-_selection['frequency'][0], 1)\n",
    "        num_classes = 2\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=(2, 2), activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    else:\n",
    "        param = {'classifier': 'adaBoost'}\n",
    "        print(f'selection={selection},   conv2d={conv2d},   classifier={param}')\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        return  AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary here to test CNN\n",
    "x_ = np.array(np.split(x, selection['time'][1]-selection['time'][0], axis=1)).transpose((1, 0, 2))\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 6:\n",
    "        y[i] = 1\n",
    "temp = np.zeros((y.size, 2))\n",
    "temp[np.arange(y.size), y] = 1\n",
    "y = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6931 - accuracy: 0.5322\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6929 - accuracy: 0.5414\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6924 - accuracy: 0.5524\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6914 - accuracy: 0.5928\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6902 - accuracy: 0.5945\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6881 - accuracy: 0.5988\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6855 - accuracy: 0.5990\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "clf = _set_clf('CNN', selection, conv2d)\n",
    "\n",
    "clf.fit(x_, y, epochs=10)\n",
    "\n",
    "# Metrics\n",
    "y_pred = clf.predict(x_)\n",
    "score = _score(y, y_pred)\n",
    "print(f'{score = :.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch  1/7: 100%|██████████| 199/199 [00:02<00:00, 79.34it/s]\n",
      "Batch  2/7: 100%|██████████| 199/199 [00:02<00:00, 86.93it/s]\n",
      "Batch  3/7: 100%|██████████| 199/199 [00:02<00:00, 94.36it/s]\n",
      "Batch  4/7: 100%|██████████| 199/199 [00:02<00:00, 97.29it/s]\n",
      "Batch  5/7: 100%|██████████| 198/198 [00:02<00:00, 97.37it/s]\n",
      "Batch  6/7: 100%|██████████| 198/198 [00:01<00:00, 99.06it/s] \n",
      "Batch  7/7: 100%|██████████| 198/198 [00:02<00:00, 93.97it/s]\n"
     ]
    }
   ],
   "source": [
    "mode = 'validation'\n",
    "batch_size = 100\n",
    "\n",
    "# Init reader and x, y for the case of use_all_data = True\n",
    "reader = Reader(batch_size=batch_size * len(condition))\n",
    "hdf5_batch_iterator = reader.read_hdf5(session, mode, condition, random=False, verbose=True)\n",
    "x_val, y_val = _init_x_y(hdf5_batch_iterator, selection)\n",
    "\n",
    "pointer = 0\n",
    "for x_batch, y_batch in hdf5_batch_iterator:\n",
    "    x_batch = _feature_selection(x_batch, selection, conv2d, parameter)  # (n_exams, n_leads, n_pars, n_features)\n",
    "    x_val[pointer:pointer + len(x_batch)] = x_batch\n",
    "    y_val[pointer:pointer + len(x_batch)] = y_batch\n",
    "    pointer += len(x_batch)\n",
    "\n",
    "    # x = x_batch\n",
    "    # y = y_batch\n",
    "    # break\n",
    "\n",
    "# lighted ram load\n",
    "del x_batch, y_batch\n",
    "\n",
    "# Aggregate all exams in one batch (can only be possible if there is selection or low memory usage)\n",
    "x_val = x_val[:, 0, 0, :]\n",
    "\n",
    "# temporary here to test CNN\n",
    "x_val = np.array(np.split(x_val, selection['time'][1]-selection['time'][0], axis=1)).transpose((1, 0, 2))\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i] == 6:\n",
    "        y_val[i] = 1\n",
    "temp = np.zeros((y_val.size, 2))\n",
    "temp[np.arange(y_val.size), y_val] = 1\n",
    "y_val = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 6ms/step\n",
      "score = 72.52\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "y_pred = clf.predict(x_val)\n",
    "score = _score(y_val, y_pred)\n",
    "print(f'{score = :.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
